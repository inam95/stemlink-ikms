"""LangGraph orchestration for the linear multi-agent QA flow."""

from functools import lru_cache
from typing import Any, AsyncGenerator, Dict
import asyncio
from concurrent.futures import ThreadPoolExecutor

from langgraph.constants import END, START
from langgraph.graph import StateGraph

from .agents import retrieval_node, summarization_node, verification_node
from .state import QAState

def create_qa_graph() -> Any:
  """Create and compile the linear multi-agent QA graph.

  The graph executes in order:
  1. Retrieval Agent: gathers context from vector store
  2. Summarization Agent: generates draft answer from context
  3. Verification Agent: verifies and corrects the answer

  Returns:
    Compiled graph ready for execution.
  """

  builder = StateGraph(QAState)

  builder.add_node("retrieval", retrieval_node)
  builder.add_node("summarization", summarization_node)
  builder.add_node("verification", verification_node)

  builder.add_edge(START, "retrieval")
  builder.add_edge("retrieval", "summarization")
  builder.add_edge("summarization", "verification")
  builder.add_edge("verification", END)

  return builder.compile()

@lru_cache(maxsize=1)
def get_qa_graph() -> Any:
  """Get the compiled QA graph instance (singleton via LRU cache)."""
  return create_qa_graph()

def run_qa_flow(question: str) -> Dict[str, Any]:
  """Run the complete multi-agent QA flow for a question.

  This is the main entry point for the QA system. It:
  1. Initializes the graph state with the question
  2. Executes the linear agent flow (Retrieval -> Summarization -> Verification)
  3. Extracts and returns the final results

  Args:
    question: The user's question about the vector databases paper.

  Returns:
    Dictionary with keys:
    - `answer`: Final verified answer
    - `draft_answer`: Initial draft answer from summarization agent
    - `context`: Retrieved context from vector store
  """

  graph = get_qa_graph()

  initial_state = {
    "question": question,
    "context": None,
    "draft_answer": None,
    "answer": None,
  }

  final_state = graph.invoke(initial_state)

  return final_state

async def stream_qa_flow(question: str) -> AsyncGenerator[str, None]:
  """Stream the multi-agent QA flow for a question, yielding tokens as they're generated.

  This async generator streams tokens from the verification agent (final answer) as they
  are generated by the LLM. It filters out tokens from other nodes (retrieval, summarization)
  to only return the final answer to the user.

  Note: Due to LangGraph's streaming behavior, we use the sync .stream() method
  in a thread pool executor to enable async streaming for FastAPI.

  Args:
    question: The user's question about the vector databases paper.

  Yields:
    String tokens from the verification agent's LLM output.
  """


  graph = get_qa_graph()

  initial_state = {
    "question": question,
    "context": None,
    "draft_answer": None,
    "answer": None,
  }

  # Use ThreadPoolExecutor to run sync stream() in a thread
  # LangGraph's sync stream() properly streams tokens, but astream() doesn't for our use case
  def _sync_stream():
    for msg, metadata in graph.stream(initial_state, stream_mode="messages"):
      # Only yield tokens from the verification node (final answer)
      if metadata.get("langgraph_node") == "verification" and msg.content:
        yield msg.content

  # Convert sync generator to async
  loop = asyncio.get_event_loop()
  with ThreadPoolExecutor(max_workers=1) as executor:
    iterator = _sync_stream()
    while True:
      try:
        # Run next() in thread pool to avoid blocking
        token = await loop.run_in_executor(executor, next, iterator, StopIteration)
        if token is StopIteration:
          break
        yield token
      except StopIteration:
        break